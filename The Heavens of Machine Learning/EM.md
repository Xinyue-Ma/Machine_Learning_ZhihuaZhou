# 期望最大化算法 Expectation Maximization (EM)

最近在回顾机器学习的一些经典方法，顺带推推公式。检验自己是否充分理解一个方法最好的方式就是复述出来，看能否讲清楚。这是这个系列的第二篇，选了 EM 算法。

EM 算法是在包含隐变量的问题中进行参数估计的一种迭代式优化算法，网上写的也非常多，大部分是基于 PRML 来讲的，但因为书中跳过了一些细节问题，所以初学时可能会有些困扰，始终掌握不了 EM 算法的本质。所以这篇文章会先从 EM 的推导出发，讲清楚 EM 的本质，从狭义 EM 推广到广义 EM，最后再结合使用高斯混合模型（Gaussian Mixture Model，GMM）求解聚类问题的例子来捋清使用 EM 的方法。

EM 用于解决概率生成模型的问题。

EM 算法是在包含隐变量的问题中进行参数估计的一种迭代式优化算法。为什么要有隐变量呢？其实我们是引入了一种归纳偏置。由于 $P(X)$ 可能非常复杂，直接去学太难了，我们就假设存在一些隐变量 $Z$ 决定了 $X$ 的生成过程，这样就能把问题进行分解，去学习相对简单的 $P(Z)$ 和 $P(X|Z)$。在真实世界中这样的例子很多，比如研究国内吃辣人群的分布 $P(X)$，由于我国幅员辽阔，每个地方都有爱吃辣和不爱吃辣的人，所以 $P(X)$ 超级复杂，但如果我们引入一个隐变量 $Z$ 表示每个人所属的省份，学习 $P(X|Z)$ 就会简单许多。 不过很多时候，隐变量 $Z$ 不一定有一个现实含义，它也可能是抽象的，未知的概念。



